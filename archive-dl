#!/bin/bash

# TODO: 
# - be able to use comments in url file
# - Group Archives

# - remove completed downloads from the url,
#   helps when restarting downloads.

# - loading stuff to menu is slow, can it be faster

# - Tidy up menu entries
# - Better way of detecting invalid urls in update_list()

# - option to choose what to do with the archive, either to play (media file) or etc.

# - Help Menu (Very Low Prioraty)

# TODO: add caching layer for every function that has expensive loops

# TODO: Things to be decide
# - make the script to be able to go into folder in archives
# - convert to different language

set -o pipefail

# Directories and File Constants 
PRG_DIR="$XDG_CONFIG_HOME/ArchiveDownloader"
ARCHV_CSV_DIR="$PRG_DIR/ArchiveData"

DWNLD_DIR="$PRG_DIR/Downloads"
INCMPLT_DWNLD_DIR="$DWNLD_DIR/.incomplete"

ARCHV_CSV_DIR="$PRG_DIR/ArchiveData"
ARCHV_URL_FILE="$PRG_DIR/archive.urls"

DWNLD_DIR_NAME="DWNLD-$(date '+%F@%H:%M:%S')"
DWNLD_URL_NAME="DWNLD-$(date '+%F@%H:%M:%S').urls"

CUR_DWNLD_DIR="$DWNLD_DIR/$DWNLD_DIR_NAME"
CUR_DWNLD_URL="$CUR_DWNLD_DIR/$DWNLD_URL_NAME"
GROUP_CACHE="$PRG_DIR/.group_cache"

# Global Lists
SEL_ARCHV=()
SEL_FILES=()
QUERY_RESULT=()
mapfile -d $'\0' DATABASE < \
    <(find "$ARCHV_CSV_DIR" -name "*.csv" -type f -print0)

# aria2c settings
DRY_RUN="false"
MAX_RETRIES="0"
CONCURRENT_DWNLD="1"

# Create the directories if they doesn't exists
[ ! -f "$GROUP_CACHE" ] && touch "$GROUP_CACHE"
[ ! -d "$PRG_DIR" ] && mkdir -p "$PRG_DIR"
[ ! -d "$DWNLD_DIR" ] && mkdir -p "$DWNLD_DIR"
[ ! -d "$ARCHV_CSV_DIR" ] && mkdir -p "$ARCHV_CSV_DIR"
[ ! -d "$INCMPLT_DWNLD_DIR" ] && mkdir -p "$INCMPLT_DWNLD_DIR"

notify_msg(){ notify-send "$1" "$2"; }
print_msg(){ printf "%s\n" "$1" 2>/dev/null; }
print_exit(){ clear && print_msg "$1" && exit "$2"; }

# read from an archive ".csv" file, return metadata
get_csv_data()
{
    arg="$1"
    csv="$2"

    [ ! -f "$csv" ] && print_exit "file named $csv not found" "1"
    case ${arg} in
        name) 
            val="$( tail -n 1 "$csv" | awk -F "," '{print $1}' )"
            [ -z "$val" ] && val="$(basename "${csv%%.csv}")"
            ;;
        url) val="$( tail -n 1 "$csv" | awk -F "," '{print $2}' )";;
        date) val="$( tail -n 1 "$csv" | awk -F "," '{print $3}' )";;
        group) val="$( tail -n 1 "$csv" | awk -F "," '{print $4}' )";;
        group_index) val="$( tail -n 1 "$csv" | awk -F "," '{print $5}' )";;
        *) print_exit "unknown argument $arg" "1"
    esac

    echo "$val"
}

update_csv_data()
{
    tag="$1"
    val=$2
    csv="$3"

    [ ! -f "$csv" ] && print_exit "file named $csv not found" "1"

    # get the old data
    name="$(get_csv_data "name" "$csv")"
    url="$(get_csv_data "url" "$csv")"
    date="$(get_csv_data "date" "$csv")"
    group="$(get_csv_data "group" "$csv")"
    group_index="$(get_csv_data "group_index" "$csv")"

    # set the new data
    case ${tag} in
        name)name=$val;;
        url)url=$val;;
        date)date=$val;;
        group)group=$val;;
        group_index)group_index=$val;;
        *) print_exit "unknown target $tag" "1"
    esac

    file_content="$(cat "$csv")"

    # old data line to be changed
    old_data="$(tail -n 1 "$csv")"
    # format the new data
    new_data="$name,$url,$date,$group,$group_index,NULL"

    # overwrite the old file
    echo "${file_content/$old_data/$new_data}" > "$csv"
}

inputbox_menu()
{
    question="$1"
    default_val="$2"
    answ="$(dialog --inputbox --stdout "$question" 0 0 "$default_val")"
    stat="$?"
    [ $stat -eq 0 ] && echo "$answ"
}

edit_archv_name()
{
    while (true); do
        # Files to choose
        opts=()
        for csv in "${DATABASE[@]}"; do
            name="$(get_csv_data "name" "$csv")"
            opts+=("$csv")
            opts+=("$name")
        done
        
        # Select a file
        sel_csv="$(dialog --no-tags --menu --stdout "what to change" 0 0 0 "${opts[@]}")"
        sel_csv_stat="$?"

        [ "$sel_csv_stat" -gt 0 ] && break
        [ -z "$sel_csv" ] && continue
        
        # get the old name, and get the user input for the new one
        old_name="$(get_csv_data "name" "$sel_csv" )"
        new_name="$(inputbox_menu "Rename the Archive" "$old_name")"
        
        # in case the user didn't enter anything
        [ -z "$new_name" ] && new_name="$old_name"
        
        # change the name
        update_csv_data "name" "$new_name" "$sel_csv"
    done
}

# TODO: Need to also edit archive.url
# edit_archv_url()
# {
#
# }

edit_archv_group()
{
    sel_archv_opts=()

    while (true); do

        # Files to choose
        for csv in "${DATABASE[@]}"; do
            name="$(get_csv_data "name" "$csv")"
            group="$(get_csv_data "group" "$csv")"
            group_index="$(get_csv_data "group_index" "$csv")"

            sel_archv_opts+=("$csv")
            sel_archv_opts+=("[$group: $group_index] $name")
            sel_archv_opts+=("OFF")
        done

        # choose the file
        sel_archv=("$(dialog --no-tags --reorder \
            --buildlist --stdout "Select Archives to be Grouped" 0 0 0 "${sel_archv_opts[@]}")")
        sel_archv_stat="$?"
        [ "$sel_archv_stat" -gt 0 ] && break
        [ -z "${sel_archv[*]}" ] && continue

        [ -f "$GROUP_CACHE" ] && rm -f "$GROUP_CACHE"
        
        # enter the intended group name
        group_name="$(inputbox_menu "Name of the Group" "NULL")"
        group_name_stat="$?"
        [ "$group_name_stat" -gt 0 ] && break
        [ -z "$group_name" ] && continue
       
        # change the group names in each file
        count=0
        for sel_csv in ${sel_archv[@]}; do
            count="$((count + 1))"
            update_csv_data "group" "$group_name" "$sel_csv"
            update_csv_data "group_index" "$count" "$sel_csv"
        done
    done
}

edit_archv_menu()
{
    while (true); do
        opts=(0 "Edit Archives Names" 1 "Edit Url" 2 "Create Archive Groups")
        menu="$(dialog --no-tags --menu --stdout "what to change" 0 0 0 "${opts[@]}")"
        stat="$?"
        [ "$stat" -gt 0 ] && break
        case ${menu} in
            0) edit_archv_name;;
            1) edit_archv_url;;
            2) edit_archv_group;;
        esac
    done
}

# select archive data to access
sel_archive()
{
    opts=()
    group_list=()
    for csv in "${DATABASE[@]}"; do
        # skip if file has only 1 line
        [ "$(wc -l "$csv" | awk '{print $1}')" -eq 1 ] && continue

        # get the archive name 
        name="$(get_csv_data "name" "$csv")"

        # Groups
        group="$(get_csv_data "group" "$csv")"
        [ -z "$group" ] && group="NULL"

        if [ "$group" != "NULL" ]; then

            # skip groups that has been processed
            grep -q "$group" <<< "${group_list[*]}" && continue

            name="$group"

            # add to the list of processed groups
            group_list+=("$group")

            # read cache file if exist 
            if grep -q "$group" "$GROUP_CACHE"; then
                csv="$(grep "$group" "$GROUP_CACHE" | cut -d "," -f2)"
            elif ! grep -q "$group" "$GROUP_CACHE"; then
                group_csv=()
                group_csv_files=()
                count=0

                # look in entire database for archives 
                # that is in the same group
                for file in "${DATABASE[@]}"; do

                    [ "$(get_csv_data "group" "$file")" != "$name" ] && continue

                    # To make the csv appear in the correct order
                    index="$(get_csv_data "group_index" "$file")"

                    # exit if the csv file has no index
                    [ -z "$index" ] && print_exit "$file, has no group index" "1"
                    
                    # store the files by index
                    declare -A group_csv+=(["$index"]="$file")
                    
                    # count the number of archives in the group
                    count=$((count + 1))
                done
                
                # append the csv file according to it's index
                for (( i=0; i<=count; i++ )); do
                    group_csv_files+=("${group_csv[$i]}")
                done

                csv="${group_csv_files[*]}"

                # write to cache file...
                echo "$name,$csv" >> "$GROUP_CACHE"
            fi

        fi

        opts+=("$csv")
        opts+=("$name")
        opts+=("OFF")
    done
    
    [ -z "${opts[*]}" ] && print_exit "cancelled..." "1"

	SEL_ARCHV=("$(dialog --title " Archive Selection Menu " --reorder \
        --no-tags --separate-output --visit-items --buildlist \
        --stdout "Choose a Archive" 0 0 0 "${opts[@]}")")
    stat="$?"

    [ "$stat" -gt 0 ] && print_exit "cancelled..." "1"
}

search_archv()
{
    in_search=1
    opts=()
    grep_result=""

    query="$(inputbox_menu "Look for what?" "")"

    [ -z "$query" ] && in_search=0

    if [ $in_search -eq 1 ]; then 

        for archv in ${SEL_ARCHV[@]}; do
            csv_data="$(head -n -1 "$archv")"
            
            if [ $in_search -eq 1 ] && grep -q "$query" <<< "$csv_data"; then
                    # re-set csv_data to be query result
                    grep_result="$(grep "$query" <<< "$csv_data")"

            # if grep returns nothing 
            elif [ $in_search -eq 1 ] && ! grep -q "$query" <<< "$csv_data"; then
                    continue
            fi

            [ -z "$grep_result" ] && break 

            archv_url="$(get_csv_data "url" "$archv")"
            while IFS=',' read -r file_url file_name upload_date file_size
            do
                opt_stat="OFF"

                if [ -n "${SEL_FILES[*]}" ]; then
                    for sel_file in ${SEL_FILES[@]}; do
                        if [ "$sel_file" == "$archv_url/$file_url" ]; then
                            opt_stat="ON"
                        fi
                    done
                fi

                if [ -n "${QUERY_RESULT[*]}" ]; then
                    for search_result in ${QUERY_RESULT[@]}; do
                        if [ "$search_result" == "$archv_url/$file_url" ]; then
                            opt_stat="ON"
                        fi
                    done
                fi

                opts+=("$archv_url/$file_url")
                opts+=("- $file_name [$file_size] [$upload_date]") 
                opts+=("$opt_stat")
            done <<< "$grep_result"
        done

        if [ -n "${opts[*]}" ]; then

            search_result="$(dialog --title "Search Results" \
                --reorder --no-tags --separate-output --checklist \
                --stdout "Select Files to Download" 0 0 0 "${opts[@]}")"
            stat="$?"

            if [ "$stat" -eq 0 ] && [ -n "$search_result" ]; then
                QUERY_RESULT+=("$search_result")
            else
                dialog --title " Canceled " --msgbox "Canceling Search" 0 0
            fi
        else
            dialog --title " Nothing Found! " \
                --msgbox "Nothing related to '$query' Found!" 0 0
        fi

    fi

    in_search=0
}

sel_files()
{
    # TODO: Different Dialog Menu for Searches "Checklist" maybe? [x]
    # TODO: after searches, set selected to be "ON" in file selection dialog [] (TESTING)
    search_files=0

    while (true); do

        [ -z "${SEL_ARCHV[*]}" ] && print_exit "cancelled..." "1"
        archv_names=()
        opts=()
        csv_data=""

        # TODO: loading multiple files from multiple archives is slow
        for archv in ${SEL_ARCHV[@]}; do

            [ "$(wc -l "$archv" | awk '{print $1}' )" -lt 2 ] && continue

            csv_data="$(head -n -1 "$archv")"

            # get the archive name and url
            archv_names+=(" $(get_csv_data "name" "$archv") ")
            archv_url="$(get_csv_data "url" "$archv")"

            # get data from each line in the csv file
            while IFS=',' read -r file_url file_name upload_date file_size
            do
                # generate options for the dialog menu
                
                # default state
                opt_stat="OFF"
                
                # re-select selected files after searches
                if [ -n "${SEL_FILES[*]}" ]; then
                    for sel_file in ${SEL_FILES[@]}; do
                        if [ "$sel_file" == "$archv_url/$file_url" ]; then
                            opt_stat="ON"
                        fi
                    done
                fi

                # Process search result
                if [ -n "${QUERY_RESULT[*]}" ]; then
                    for search_result in ${QUERY_RESULT[@]}; do
                        if [ "$search_result" == "$archv_url/$file_url" ]; then
                            opt_stat="ON"
                        fi
                    done
                fi

                # add them to the argument array
                opts+=("$archv_url/$file_url")
                opts+=("- $file_name [$file_size] [$upload_date]") 
                opts+=("$opt_stat")

            done <<< "$csv_data"
        done

        # reset to false
        [ $search_files ] && search_files=0

        [ -z "${opts[*]}" ] && print_exit "cancelled..." "1"

        SEL_FILES=("$(dialog --title "${archv_names[*]}" --reorder --no-tags \
            --separate-output --extra-button --extra-label "Search" --visit-items \
            --buildlist --stdout "Choose Files to Download" 0 0 0 "${opts[@]}")")
        stat="$?"
        
        if [ "$stat" -eq 0 ]; then

            # exit if nothing if nothing is selected
            [ -z "${SEL_FILES[*]}" ] && print_exit "cancelled..." "1"

            # otherwise, break out of the loop
            break

        # if "search" button is pressed
        elif [ "$stat" -eq 3 ]; then
            # begin search
            search_archv

            # if nothing is entered re-run the loop
            [ -z "$QUERY_RESULT" ] && continue
            
            # begin search
            search_files=1
        else
            print_exit "cancelled..." "1"
        fi
    done
}

# set aria2c settings
download_settings_menu()
{
    while (true)
    do
        opts=(\
            0 "Max Retries ($MAX_RETRIES)" \
            1 "Concurrent Downloads ($CONCURRENT_DWNLD)" \
            2 "Dry Run ($DRY_RUN)"\
        )

        menu="$(dialog --title " aria2c settings " --cancel-label "Done" \
            --reorder --no-tags --menu --stdout "Set an option?" 0 0 0 "${opts[@]}")"
        stat="$?"

        # If "Done" Is Pressed
        [ "$stat" -gt 0 ] && break

	    case ${menu} in
            0) MAX_RETRIES="$(dialog --rangebox --stdout "Maximum Retries" 0 0 0 100 "$MAX_RETRIES")";;
            1) CONCURRENT_DWNLD="$(dialog --rangebox --stdout "Number of Files Downloaded at a time" 0 0 1 100 "$CONCURRENT_DWNLD")";;
            2) 
                dialog --yesno "Dry Run? ($DRY_RUN)" 0 0 
                answ="$?"
                if [ "$answ" -ne 0 ]; then
                    DRY_RUN="false" 
                else
                    DRY_RUN="true"
                fi
            ;;
        esac
    done
}

# Download the files
download_files()
{
    sel_archive
    sel_files

    [ -z "${SEL_FILES[*]}" ] && print_exit "cancelled..." "1"

    # confirm download
	dialog --title " Continue to Download? " \
        --yesno "Files to Download: $(printf "%s\n" "${SEL_FILES[@]}" | wc -l)" 0 0
	answ="$?"

	[ "$answ" -ne 0 ] && print_exit "cancelled..." "1"

    download_settings_menu

    # create the download directory for the files
    [ ! -d "$CUR_DWNLD_DIR" ] && mkdir -p "$CUR_DWNLD_DIR"

    # Write the selected file's Url to a file
    printf "%s\n" "${SEL_FILES[@]}" >> "$CUR_DWNLD_URL"

    # begin the download
    clear
    if [ "$DRY_RUN" == "true" ]; then
        aria2c --dry-run "true" -m "$MAX_RETRIES" -j "$CONCURRENT_DWNLD" \
            -d "$CUR_DWNLD_DIR" -i "$CUR_DWNLD_URL"
        dwnld_stat="$?"

        # Delete auto-generated files if dry run is selected
        rm -rf "$CUR_DWNLD_DIR"
    else
        aria2c -m "$MAX_RETRIES" -j "$CONCURRENT_DWNLD" \
            -d "$CUR_DWNLD_DIR" -i "$CUR_DWNLD_URL"
        dwnld_stat="$?"

        if [ "$dwnld_stat" -gt 0 ]; then
            # if any error occured move the download directory to 
            # the incomplete downloads directory
            mv "$CUR_DWNLD_DIR" "$INCMPLT_DWNLD_DIR"
            print_exit "Moved incomplete downloads to $INCMPLT_DWNLD_DIR" "1"
        fi
    fi
}

update_list()
{
    clear

    [ ! -f "$ARCHV_URL_FILE" ] && print_exit "Error: archive.url not found!" "1"

	dialog --yesno "Update Existing Files?" 0 0
	answ="$?"

    print_msg "Updating Database..."

    a_count=0
    f_count=0
	while IFS=" " read -r url name ; do

        ! grep -q "https://archive.org/download/" <<< "$url" && continue

		csv_name="$(cut -d "/" -f5 <<< "$url").csv"

        if [ "$answ" -ne 0 ]; then
            # don't update if the csv already existed
            [ -f "$ARCHV_CSV_DIR/$csv_name" ] && continue
        fi

        # skip empty lines
        [ -z "$url" ] && continue

        a_count=$((a_count + 1))

        # get the url html
		print_msg "#### Fetching data from '$url'..." 
		fetch_list="$(curl --silent --fail "$url")"
		stat="$?"

		if [ "$stat" -gt 0 ]; then
			print_msg "WARNING: An error occured when downloading $url, exited with error code $stat"
            f_count=$((f_count + 1))
            continue
        fi

        # Convert html to csv
		print_msg "> Generating csv from $url"
		tr -d '\n' <<< "$fetch_list" | \
            grep -E -o '<tbody>.*</tbody>' | \
			sed -r 's/<td><a href=//g' | \
            sed -r 's/<\/tr>/\n/g; s/  //g' | \
			sed -r 's/<(\/td|\/a|tr.)>//g; s/\(<a.*\)//g; s/,/_/g' | \
			sed -r 's/<td>/,/g; s/">/,/g; s/ ,/,/g' | \
			tr -d '"' | head -n -1 | tail -n +2 > "$ARCHV_CSV_DIR/$csv_name"
		stat="$?"

		if [ "$stat" -eq 0 ]; then
            # append to the last line the metadata, name of archive, url, and last modified.
            echo "$name,$url,$(date +'%d-%b-%Y %H:%M'),NULL,NULL,NULL," >> "$ARCHV_CSV_DIR/$csv_name"
			print_msg "> Success... Generated $ARCHV_CSV_DIR/$csv_name"
		else
			print_msg "> Error: Failed to generate csv from $url"
            f_count=$((f_count + 1))
		fi

	done <<< "$(cat "$ARCHV_URL_FILE")"

    print_msg "Process completed, $f_count Failed, out of $a_count"
}

continue_downloads()
{
    # TODO:
    # PRE-DOWNLOAD
    # - Check for incomplete downloads
    # - Ask user which to download
    # BEGIN DOWNLOAD
    # - If completed, move to download dir
    # - If failed, prompt user of failure, exit

    mapfile -d $'\0' dwnld_dir < <(find "$INCMPLT_DWNLD_DIR" -name "DWNLD-.*" -type d -print0)

    [ -z "${dwnld_dir[*]}" ] && print_exit "Nothing to Download" "1"

    incmplt_dwnld_menu_opts=()

    for dir in "${dwnld_dir[@]}"; do
        ! ls "$dir" | grep -Eq ".*.urls" && continue
        
        url_file="$(find "$dir" -maxdepth 1 -name "*.urls" -type f | head -n 1)"

        incmplt_dwnld_menu_opts+=("$dir/$url_file $dir")
        incmplt_dwnld_menu_opts+=("$(basename $dir)")
    done
    
    while (true); do
        incmplt_dwnld_menu="$(dialog --no-tags -menu \
            --stdout "Select an Incomplete Download" 0 0 0 "${incmplt_dwnld_menu_opts[@]}")"
        incmplt_dwnld_menu_stat="$?"

        [ "$incmplt_dwnld_menu_stat" -gt 0 ] & exit 1

        aria2c -C --dry-run "$DRY_RUN" -m "$MAX_RETRIES" -j "$CONCURRENT_DWNLD" \
            -d "$(cut -d " " -f2- <<< $incmplt_dwnld_menu)" -i "$(cut -d " " -f1- <<< $incmplt_dwnld_menu)"
        dwnld_stat="$?"

        # TODO: copy_paste!
        # if [ "$DRY_RUN" == "false" ]; then
        #     if [ "$dwnld_stat" -eq 0 ]; then
        #         # TODO: What to do in case a download succeed
        #     else
        #         # TODO: What to do in case a download fails and etc
        #     fi
        # fi
    done

    # check if there is any incomplete downloads
    # show a menu of every incomplete download
    #   if user select any, start the download
}

main_menu(){
	opts=( 1 "Download Files" 2 "Update Lists" 3 "Continue Downloads" 4 "Edit Archives")
	main_menu="$(dialog --title " Archive.org Downloader " --reorder --no-tags \
		--menu --stdout "What to do?" 0 0 0 "${opts[@]}")"
	case ${main_menu} in
		1) download_files ;;
		2) update_list;;
		3) continue_downloads ;;
        4) edit_archv_menu;;
	esac
}

main_menu
